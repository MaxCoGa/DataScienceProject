{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "#Can be very helpful to notice any imbalance in classes\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sourced from https://www.postgresqltutorial.com/postgresql-python/connect/\n",
    "def config(filename='psql_sample.ini', section='postgresql'):\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename) \n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the configuration file as a python dictionary\n",
    "cfg = config()\n",
    "\n",
    "#Establish the connection and create a cursor to the database\n",
    "try:\n",
    "    print(\"Here's an attempt to connect to the database\")\n",
    "    conn = psycopg2.connect(**cfg)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Look's like it was a success\")\n",
    "    \n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT QUERY\n",
    "try:\n",
    "    #Lets get our data \n",
    "    cursor.execute(\"SELECT age_group,gender,status,holiday,phu,daily_high,daily_low,rain_amount,snow_amount,retail_and_recreation_percentage,grocery_and_pharmacy_percentage,workplaces_percentage,residential_percentage,parks_percentage,is_fatal,is_resolved,is_unresolved from data_mart.fact_table as fact inner join data_mart.mobility_dimension as mobility on fact.mobility_key=mobility.mobility_key inner join data_mart.weather_dimension as weather on fact.weather_key=weather.weather_key inner join data_mart.patient_dimension as patient on fact.patient_key=patient.patient_key inner join data_mart.special_measures_dimension as measures on fact.special_measures_key=measures.special_measures_key inner join data_mart.phu_location_dimension as phu on fact.phu_location_key=phu.phu_location_key inner join data_mart.reported_date_dimension as date on fact.reported_date_key=date.reported_date_key\") \n",
    "\n",
    "    #Get the complete result set. It will be a list of tuples where each tuple is a row from the result set\n",
    "    result_list = cursor.fetchall()\n",
    "        \n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure to run this cell at the end of all your experiments to close all connections\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, \n",
    "result_df = pd.DataFrame(result_list, columns=[\"age_group\",\"gender\",\"holiday\",\"phu\",\"status\",\"daily_high\",\"daily_low\",\"rain_amount\",\"snow_amount\", \"parks_percentage\", \"retail_and_recreation_percentage\",\"grocery_and_pharmacy_percentage\",\"workplaces_percentage\",\"residential_percentage\",\n",
    "                                               \"is_fatal\", \"is_resolved\",\"is_unresolved\"])\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "#One-hot encoding:\n",
    "new_result = pd.get_dummies(result_df)\n",
    "new_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing null values :\n",
    "result_df[\"park\"].fillna(result_df[\"park\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing data:\n",
    "transform_data = result_df[[\"retail_rec\",\"grocery_pharm\",\"parks\",\"transit\",\"workplaces\",\"residential\",\"daily_high\",\"daily_low\",\"rain_amount\",\"snow_amount\"]]\n",
    "X_normalized=preprocessing.normalize(transform_data,norma='12')\n",
    "normalize_part=pd.DataFrame(X_normalized,columns=transform_data.columns)\n",
    "non_numerical=result_df[[\"status\",\"age_group\",\"is_fatal\",\"is_resolved\",\"is_unresolved\"]]\n",
    "result_data=pd.concat([non_numerical,normalize_part],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling of majority classes: \n",
    "from imblearn.under_sampling import NearMiss\n",
    "X = new_rsult.values\n",
    "y = result_df[\"is_resolved\"]\n",
    "undersample = NearMiss(version=1,n_neighbors=3)\n",
    "X-under, y_under = undersample.fir_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART B\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "import datetime\n",
    "#Gradient Boosting\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_under, y_under, test_size=0.33, shuffle=True, stratify=y_under)\n",
    "a = datetime.datetime.now()\n",
    "classifier = GradientBoostingClassifier(n_estimators=20, learning_rate=0.75, max_features=2, max_depth=2, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "b = datetime.datetime.now()\n",
    "c = b - a\n",
    "print('time in milliseconds')\n",
    "print(c.total_seconds() * 1000)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "accuracy = accuracy_score(predictions, y_test) * 100\n",
    "recall = recall_score(predictions, y_test) * 100\n",
    "precision = precision_score(predictions, y_test) * 100\n",
    "\n",
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.2, shuffle=True, stratify = y_under)\n",
    "\n",
    "#create and fit random forest\n",
    "rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_test) * 100\n",
    "recall = recall_score(y_pred, y_test) * 100\n",
    "precision = precision_score(y_pred, y_test) * 100\n",
    "\n",
    "print(\"Accuracy of Random Forest: {:.2f} %\".format(accuracy))\n",
    "print(\"Precision of Random Forest: {:.2f} %\".format(precision))\n",
    "print(\"Recall of Random Forest: {:.2f} %\".format(recall))\n",
    "print(\"Random forest construction time: \", (round((end - start), 4) * 1000), \"milliseconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Algorithm:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.2, shuffle=True, stratify = y_under)\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_pred,y_test) * 100  \n",
    "recall = recall_score(y_pred, y_test) * 100 \n",
    "precision = precision_score(y_pred, y_test) * 100\n",
    "\n",
    "print(\"Accuracy of Decision Tree{:.2f} %\".format(accuracy))\n",
    "print(\"Recall of Decision Tree{:.2f} %\".format(recall))\n",
    "print(\"precision of Decision Tree{:.2f} %\".format(precision))\n"
   ]
  }
 ]
}